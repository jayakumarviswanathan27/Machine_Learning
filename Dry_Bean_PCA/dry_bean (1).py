# -*- coding: utf-8 -*-
"""Dry_Bean.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1go_DtIDJxOMGk_vuLChkMiw_W7tVJtRe
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import  MinMaxScaler

from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split

df=pd.read_csv("Dry_Bean.csv")
df

df.shape

df.info()

df.describe()

x = df.drop('Class', 1)
y = df["Class"]

x

y

encoder1= LabelEncoder()
 y = encoder1.fit_transform(y)

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=2)

from sklearn.linear_model import LogisticRegression
lr_model = LogisticRegression()
lr_model.fit(x_train, y_train)

y_logistic = lr_model.predict(x_test)
y_logistic1 = lr_model.predict(x_train)

from sklearn.metrics import accuracy_score
lr_accuracy =accuracy_score(y_test,y_logistic)
lr_accuracy

accuracy_score(y_train,y_logistic1)

"""**Example of standerdization**"""

scaler = StandardScaler()
X_features = scaler.fit_transform(x)

X_features

"""**Example of minmax**"""

minmax = MinMaxScaler()
X_minmax = minmax.fit_transform(x)

X_minmax

"""**Standard scaler model** **training**"""

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(X_features, y, test_size=0.2, random_state=2)
from sklearn.linear_model import LogisticRegression
lr_model = LogisticRegression()
lr_model.fit(x_train, y_train)
y_logistic_s = lr_model.predict(x_test)
y_logistic1_s = lr_model.predict(x_train)

from sklearn.metrics import accuracy_score
lr_accuracy_s =accuracy_score(y_test,y_logistic_s)
lr_accuracy_s

accuracy_score(y_train,y_logistic1_s)

"""**pca standard scaler**"""

pca_s = PCA(n_components=6)
x_train_pca_s=pca_s.fit_transform(x_train)
x_test_pca_s=pca_s.fit_transform(x_test)

from sklearn.linear_model import LogisticRegression
lr_model = LogisticRegression()
lr_model.fit(x_train_pca_s, y_train)
y_logistic = lr_model.predict(x_test_pca_s)
y_logistic1 = lr_model.predict(x_train_pca_s)

from sklearn.metrics import accuracy_score
lr_accuracy =accuracy_score(y_test,y_logistic)
lr_accuracy

accuracy_score(y_train,y_logistic1)

"""**minmax traning and testing data**"""

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(X_minmax, y, test_size=0.2, random_state=2)
from sklearn.linear_model import LogisticRegression
lr_model = LogisticRegression()
lr_model.fit(x_train, y_train)
y_logistic = lr_model.predict(x_test)
y_logistic1 = lr_model.predict(x_train)

from sklearn.metrics import accuracy_score
lr_accuracy =accuracy_score(y_test,y_logistic)
lr_accuracy

accuracy_score(y_train,y_logistic1)

"""**pca for minmax**"""

pca = PCA(n_components=6)
x_train_pca=pca.fit_transform(x_train)
x_test_pca=pca.fit_transform(x_test)

from sklearn.linear_model import LogisticRegression
lr_model = LogisticRegression()
lr_model.fit(x_train_pca, y_train)
y_logistic = lr_model.predict(x_test_pca)
y_logistic1 = lr_model.predict(x_train_pca)

from sklearn.metrics import accuracy_score
lr_accuracy =accuracy_score(y_test,y_logistic)
lr_accuracy

accuracy_score(y_train,y_logistic1)



"""**Feature Scaling :**
Feature Scaling is done to normalize the features in the dataset into a finite range.

*   Absolute Maximum Scaling
*   Min-Max Scaling
*   Normalization
*  Standardization
*  Robust Scaling


---


**Min-Maximum Scaling**
In min-max you will subtract the minimum value in the dataset with all the values and then divide this by the range of the dataset(maximum-minimum). In this case, your dataset will lie between 0 and 1.


**Standardization**
It is a very effective technique which re-scales a feature value so that it has distribution with 0 mean value and variance equals to 1.




---
If there are a lot of outliers in the dataset it is reccomented to use Robust Scaling.Standardization does not get affected by outliers because there is no predefined range of transformed features, so it needed to be removed explicily.


---
**Feature selection** yields a subset of features from the original set of features, which are best representatives of the data. It is an exhaustive search.

**Dimensionality Reduction** as the name suggests is the process of transforming the features into a lower dimension.
It projects the data into a lower dimensionality space. That in turn can work quite well or not for your classification algorithm.

**Feature Selection**

*   Remove features with missing values
*   Feature elimination

**Dimensionality Reduction**
*   PCA










"""

